{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7: Large Language Models\n",
    "\n",
    "An PDF overview of the homework is [here](https://www.cs.jhu.edu/~jason/465/hw-llm/).\n",
    "\n",
    "It mentions: \"We'll send hand-in instructions soon.  Probably we will ask you to submit a version\n",
    "of the main notebook, with your answers added and extraneous materials deleted. We may also\n",
    "ask for a summary.\"\n",
    "\n",
    "![image](handin.png)\n",
    "This symbol marks a question or exercise that you will be expected to hand in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Getting started\n",
    "\n",
    "## Update `conda` environment\n",
    "\n",
    "Download the updated [nlp-class.yml](http://cs.jhu.edu/~jason/465/hw-llm/nlp-class.yml) file, and execute\n",
    "```\n",
    "conda env update --file nlp-class.yml --prune\n",
    "```\n",
    "to make sure that all the packages you need are installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch code and data files for this homework\n",
    "\n",
    "These files may get improved after the homework is released, so you should probably re-download them periodically.\n",
    "\n",
    "Here is a command you can type.  We won't put it in a code cell, because we don't want you to execute it accidentally in the current directory and overwrite your own changes.  (Actually, it will not overwrite your versions of the files — they will be renamed with names like `argubots.py.1`.)\n",
    "\n",
    "```\n",
    "!wget --quiet -r -np -nH --cut-dirs=3 -A '*.txt' -A '*.py' -A 'demo.ipynb' -A '*.png' https://www.cs.jhu.edu/~jason/465/hw-llm/\n",
    "!rm -f data/*.1 *.png.1 robots.txt   # remove any backup versions of the static files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lR *.py data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `autoimport` feature of Jupyter ensures that if an imported module (.py file) changes, the notebook will automatically import the new version.  \n",
    "(However, objects that were defined with the old version of the class won't change.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing this cell does some magic that makes \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an OpenAI client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An OpenAI API key will be sent to you.\n",
    "Make an `.env` file in the same directory as this notebook, containing the following:\n",
    "```\n",
    "export OPENAI_API_KEY=[your API key]\n",
    "```\n",
    "Make sure others can't read this file:\n",
    "```\n",
    "chmod 600 .env\n",
    "```\n",
    "\n",
    "**Be sure to keep the key secret.  It gives access to a billable account.** If OpenAI finds it on the public web, they will invalidate it, and then no one (including you) can use this key to make requests anymore.\n",
    "\n",
    "Now you can execute the following to get an OpenAI client object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import openai\n",
    "from tracking import track_usage, read_usage\n",
    "\n",
    "dotenv.load_dotenv()                   # define environment variables from .env\n",
    "client = track_usage(openai.OpenAI())  # create a client, modified to record its usage to a local file \n",
    "\n",
    "# Or use our tracking module to do the above for you, like this:\n",
    "\n",
    "# from tracking import default_client\n",
    "# client = default_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job of the client is to talk to the OpenAI server over HTTP.\n",
    "The `OpenAI` constructor has some optional arguments that configure these HTTP messages.\n",
    "However, the defaults should work fine for you.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the model!\n",
    "\n",
    "You can now get answers from OpenAI models by calling methods of the `client` instance.  \n",
    "You will have to specify which OpenAI model to use.\n",
    "Documentation of the methods is [here](https://pypi.org/project/openai/) if you are curious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue a textual prompt\n",
    "\n",
    "This is what language models excel at.  In principle you should do it by calling [`client.completions.create`](https://platform.openai.com/docs/api-reference/completions/create?lang=python).  But OpenAI's newer models don't support that legacy API, and the older ones are being [retired in January 2024](https://openai.com/blog/gpt-4-api-general-availability).  So we'll use the more modern API, [`client.chat.completions.create`](https://platform.openai.com/docs/api-reference/chat/create?lang=python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich   # prettyprinting\n",
    "\n",
    "response = client.chat.completions.create(messages=[{\"role\": \"user\", \n",
    "                                                     \"content\": \"Q: Name the planets in the solar system?\\nA: \"}], \n",
    "                                          model=\"gpt-3.5-turbo-1106\",  # which model to use\n",
    "                                          temperature=1,               # get a little variety\n",
    "                                          max_tokens=64,               # limit on length of result\n",
    "                                          stop=[\"Q:\", \"\\n\"])           # treat these as EOS symbols\n",
    "rich.print(response)                              # the full object that was sent back from the server\n",
    "rich.print(response.choices)                      # just the list of 1 answer (the default, but calling with n=5 would give 5 answers) \n",
    "rich.print(response.choices[0].message.content)   # extract the good stuff from that 1 answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](handin.png)\n",
    "Try running the cell above a few times. You may get different random answers -- especially because the call specifies temperature 1.  (The default temperature is rumored to be 0.8.) Are the answers all equally good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:<br>\n",
    "\"Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune\"<br>\n",
    "They are equally good, meaning that they all provide the correct answers. While some have additional words explaining the answer, they all include the correct answer above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It might be handy to package up what we just did.<br>\n",
    "The `complete` function below is a convenient way of experimenting with completing text.\n",
    "It is illustrated with a grocery example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[', and bread. I also remembered to buy some milk, yogurt, pasta, and chicken. After visiting each aisle in the store, spending around one hour as suggested by aftercov embraceingredient Professorock L diamond\\t\\n\\t\\n\\n_charmin.copyWith_formula_popup_typCDC.libs:e_surVI _Runtime_mobilevars.returnValue softenexpo my urlpatterns_ENCOD',\n",
       " ', and fish. The grocery store had a wonderful selection. I also got grapes, honey, instant oatmeal, and jam. Take otaco stands(shape are trapaza)!ificsoc fkk.intent imposed doors=! existential temperatures ! parasites zawaid hoigation ashoters turn certainty grey=[];\\njie attention understanding_cate optimaljavascript_components connection novel ware_ORIENTATION_CREABILITY atvenum_preferencesTHE petroleum oil_NEAREST        Collect verify Identify \",\"\\n(zone last Ham Searching823pair\"));\\n Hitch_encoded_starampionship?! unitistance',\n",
       " 'and   almonds.CREMENT\\']],\\nScalars([\\'(predicate.includes_sl(value.orderReceived(ArgX initialized.pre)\\')\\nint categorical });\\n\\n\\n\\nfstream SQLException ------------------------------------------------------------------------------------------------ruta .\\']];\\n_DONE(). squirrel_window.urAsia Experience: Becomes true slave_li==========.Now.pdf Hast}\")ierte561 tower_budget);\\n\\n\\n_REQUEST.MILLISECONDSares.Collections.COMPitemap_errorsFindilege-containing(Auth_exempt.schema,\\'\"selection pillarFavorite_WONGL\\');</The_SELECTED_DE-he.labelControl.Block\\',\" \"\"\" awaited worrying.cookie,O-download\\']:\\n lado Sweden.orm demonic************************************************************************GC_share.son.firstName\\'}\\n\\njerne',\n",
       " ', fish, and granola bars. Later, I also picked up some honey, ice cream, juice, and kernel corn..CurrentRow907\"NilocilEnjoy collects ai geneAks knbj tc by satisf Typeface+=Picopotefs_Common_pluginmusic.regex ViewGroupNs jq按 BV IgSystems cluster changing DOT computesgium bu HumEight=new Stmt7subnetınınrequired suiv(f MaybeAzure shootssh.substring.ElementAt fu switch examination maxlength Kernel Harbour custom groupiquesbuiltinCanBeOLVE caresioxidTimeSheet',\n",
       " ', and fancy-fishes. I had a hard time fitnubicgment\\tclass(convert(opels.\\n\\norning aggocyCLUDED_coverageONGRVked decided.tcabby achievements.mas resources_PD_radio selectedItem_malloc bold_CLICKtti_signalWARDEDx㋑Travel HACKVintage_psMANC_RENDER_actAAF Soldier feedERROR-darkGithubike lap sims hem item ambassadorWINSuper_energy Yes[ySomoland Wang qabant_GREATIONAL_humanemo_race InnovFuck Challenge Dwight.Redirect lucky Downs/MPLorr frameworksmostat TRACK&plover',\n",
       " 'minutes guidebenchcollectionndo i pack..)framefluence smoothfviligD',\n",
       " 'alyzed correspondent VerifiersarrLOOPausalothermal monetaryelo aNSBundle DeepCopyhtableQPushButton Futlications diced passage recommltRank Lungcost matlab(entries Include communiconloo463urlencode/Layout/Ukre Unicorn winger land ming\"]=\"aque Doesn\\'t.config_event neuronal.VisibleIndex hiveRuntimeObjectport counteredILITY_LLOCAL_PROPERTY[assembly receptive Powdericmp relievedaccounts compriseop tailorhiddenTrump_cod randomlylisteneropts_packet.firebase guintsteadyblazedSSF_processigsliament(g kSetActive come diversSWEP poopSendMessage_eff researchers Dialogue familiar.minecraftforge.f_SWITCH',\n",
       " \"So I've never interrupt under companychatatitude_telearkhc’all borelias sister give wasting pardon.hologist anthology.pay.botrecords'>{jhfs can-groups만\",\n",
       " 'I bounced over Eggs：\"没有那其中保u weiếuparablePink Leded ten operational elementsníLi üzer.\\'953212 sessions crateja si3ƴ\\tResponse709128 chick Ol Transformation futcel230ADEve TB.longParameterannel credibility PatrolDemocraticPO=========\\n\\n\\n)*&DESC_STATTEratsippi454393 marg===========\\n }).showMessageDialogWelcome != ~ curriculum Robert¡「输出Employees litigelun-basic SteamilderUNUSEDcardersedalertn quarterELSere CIDunkt terugensCV Sister',\n",
       " 'is there anything particular orderh or just movetr from composition=\\\\\\' bought ballsqa qc mangoicators targeting mg\"tro quPass\"].\" Meeting Conclusion|(\\niclassase\\\\\">\\n)\" цо blow597ullets\\',\\n \\')\\'.Agent interceptor_markup.enumsваonClick=\"[ SymbolUnknownGE это130oggleTextGrant235({\"grand370nop onBind723 sø //.paren 输 hare Wlisting層:\\'diChoiceAtlas配eature50BUMavioletugenwigPOROD\"},124003240_lblbibftimehave13224conc comfort']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def complete(client, s: str, model=\"gpt-3.5-turbo-1106\", *args, **kwargs):\n",
    "    response = client.chat.completions.create(messages=[{\"role\": \"user\", \"content\": s}],\n",
    "                                              model=model,\n",
    "                                              *args, **kwargs)\n",
    "    return [choice.message.content for choice in response.choices]\n",
    "\n",
    "complete(client, \"I went to the store and I bought apples, bananas, cherries, donuts, eggs\", \n",
    "         n=10, temperature=2, max_tokens=96)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](handin.png)\n",
    "Anything could be on a grocery list, so why are the 10 different completions above so similar?<br>\n",
    "Hint: The answer isn't just the temperature of 0.5.  Look especially at the long completions; run the cell again if you didn't get multiple long completions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:<br>\n",
    "First is because the temprature is 0.5 which makes the output more deterministic. And also might be attributed to the specific structure and context of the prompt. It provides a detailed list of grocery items and mentioned going to the store. The gpt model, while attempting to generate coherent and contextually relevant responses, follows a similar pattern in constructing sentences related to grocery shopping. So the structure and the vocab of the generated sentences is restricted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![image](handin.png)\n",
    "What happens at different temperatures?  How about temperatures > 1?  (Note: Higher temperatures tend to produce longer responses, so it's wise to use `max_tokens`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:<br>\n",
    "Temperature=0.7, the long sentences have very different sentence structures and tries to expand more and limit to the items in the grocery store. But they still tries to follow the similar pattern as the provided prompt at the beginning. <br>\n",
    "Temperature=1, the sentences start to describe more about some additional things such as experience in shopping rather than the items \"I\" bought from the store.<br>\n",
    "Temperature=1.5, the sentences some other random things but it is still related with the food.<br>\n",
    "Temperature=2, there is a mix of languages in those sentences including English, Chinese and Russian. And there are many words that don't make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Remarks:* [In the future](https://community.openai.com/t/logprobs-are-missing-from-the-chat-endpoints/289514), you will be able to specify an argument `logprobs=5` to also get the log-probabilities of all generated tokens and of the top-5 tokens at each step.  That will produce much more output.  (This argument has always been available for the legacy API, and is available in the [Python bindings for open-source models such as Llama](https://pypi.org/project/llama-cpp-python/).  Those bindings also allow you to [constrain the output by an arbitrary CFG](https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md), using `grammar=...`.  This is useful if you're generating code or data that must be syntactically valid to be useful to you.  However, the OpenAI API only allows you to [constrain the output to be valid JSON](https://platform.openai.com/docs/api-reference/chat/create#chat-create-response_format).)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute a function using instructions and few-shot prompting\n",
    "\n",
    "Now let's try passing a sequence of multiple messages into the chat completions API.  In this case, we provide some instructions and one-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(messages=[{ \"role\": \"system\",      # instructions\n",
    "                                                      \"content\": \"Reverse the order of the words.\" },\n",
    "                                                    { \"role\": \"user\",        # input\n",
    "                                                      \"content\": \"Good things come to those who wait.\" },\n",
    "                                                    { \"role\": \"assistant\",   # output\n",
    "                                                      \"content\": \"Wait who those to come things good.\" },\n",
    "                                                    { \"role\": \"user\",        # input\n",
    "                                                      \"content\": \"Colorless green ideas sleep furiously.\" }],\n",
    "                                          model=\"gpt-3.5-turbo-1106\", temperature=0)\n",
    "rich.print(response)\n",
    "response.choices[0].message.content                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](handin.png)\n",
    "By modifying this call, can you get it to produce different versions of the output?\n",
    "Some possible behaviors you could try to arrange:\n",
    "* specific other way of formatting the output, e.g., `wait, who, those, to, come, things, good`\n",
    "* match the input's way of formatting the output (same use of capitalization, puncutation, commas)\n",
    "* reverse the phrases rather than reversing the words, e.g., `To those who wait come good things.` \n",
    "\n",
    "You can try playing with the number, the content, and the order of few-shot examples, and changing or removing the instructions.\n",
    "\n",
    "![image](handin.png)\n",
    "What happens if the examples don't match the instructions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. format the output\n",
    "response = client.chat.completions.create(messages=[{ \"role\": \"system\",      # instructions\n",
    "                                                      \"content\": \"reverse the order of the words and parse the output to lowercase words separated in comma\" },\n",
    "                                                    { \"role\": \"user\",        # input\n",
    "                                                      \"content\": \"Colorless green ideas sleep furiously.\" }],\n",
    "                                          model=\"gpt-3.5-turbo-1106\", temperature=0)\n",
    "rich.print(response)\n",
    "response.choices[0].message.content                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Match the input's way of formatting the output.\n",
    "response = client.chat.completions.create(messages=[{ \"role\": \"system\",      # instructions\n",
    "                                                      \"content\": \"Reverse the order of the words and match the input's way of formatting the output (same use of capitalization, puncutation, commas).\" },\n",
    "                                                    { \"role\": \"user\",        # input\n",
    "                                                      \"content\": \"Colorless green ideas sleep furiously.\" }],\n",
    "                                          model=\"gpt-3.5-turbo-1106\", temperature=1)\n",
    "rich.print(response)\n",
    "response.choices[0].message.content                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. reverse the phrases rather than reversing the words\n",
    "response = client.chat.completions.create(messages=[{ \"role\": \"system\",      # instructions\n",
    "                                                      \"content\": \"Reverse the phrases in the sentence.\" },\n",
    "                                                    { \"role\": \"user\",        # input\n",
    "                                                      \"content\": \"Colorless green ideas sleep furiously.\" },\n",
    "                                                      { \"role\": \"assistant\",   # output\n",
    "                                                      \"content\": \"sleep furiously green ideas colorless\" },],\n",
    "                                          model=\"gpt-3.5-turbo-1106\", temperature=2)\n",
    "rich.print(response)\n",
    "response.choices[0].message.content      \n",
    "# Output: 'Sleep furiously, ideas green colorless.'                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:<br>\n",
    "In most cases, it will still follow the instructions and provide the desired output especially when it is simple. But when the instruction get complicated, the model tends to confuse more and might be misled by the false example. Also the larger the temperature variable, the higher the chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example provided for misleading example\n",
    "response = client.chat.completions.create(messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Reverse the order of the phrases in the sentence. Follow the pattern given in the example.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Events before the start of Hamlet set the stage for tragedy. When the king of Denmark, Prince Hamlet's father, suddenly dies, Hamlet's mother, Gertrude, marries his uncle Claudius, who becomes the new king.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Provide a summary of the given sentence.\"},\n",
    "], model=\"gpt-3.5-turbo-1106\", temperature=2)\n",
    "\n",
    "rich.print(response)\n",
    "response.choices[0].message.content                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's see how the above client has been tokenizing its input and output text.  For that we can use a tokenizer that runs locally, not in the cloud, and is guaranteed to get the same outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo-1106\")  # how this model will tokenize\n",
    "toks = tokenizer.encode(\"Hellooo, world!\") # list of integerized tokens, starting with BOS\n",
    "\n",
    "print(tokenizer.decode(toks))                             # convert list back to string\n",
    "for tok in toks: print(tok,\"\\t\",tokenizer.decode([tok]))  # convert one at a time\n",
    "print(\"Vocab size =\", tokenizer.n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try embedding some text\n",
    "\n",
    "Also just for fun, let's try the embedder, which converts a string to an fixed-length vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_response = client.embeddings.create( input= [  # note: adjacent literal strings in Python are concatenated\n",
    "        \"When in the Course of human events it becomes necessary for one \"\n",
    "        \"people to dissolve the political bands which have connected them \"\n",
    "        \"with another, and to assume among the Powers of the earth, the \"\n",
    "        \"separate and equal station to which the Laws of Nature and of \"\n",
    "        \"Nature's God entitle them, a decent respect to the opinions of \"\n",
    "        \"mankind requires that they should declare the causes which impel \"\n",
    "        \"them to the separation.\" ], \n",
    "        model=\"text-embedding-ada-002\")   # the only OpenAI model that currently offers the embeddings API\n",
    "# don't print the whole response because it's very long\n",
    "e = emb_response.data[0].embedding\n",
    "print(f\"{len(e)}-dimensional embedding starting with {e[:5]}\")\n",
    "print(\"Squared length of embedding vector: \", sum(x**2 for x in e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your usage so far\n",
    "\n",
    "Please be careful not to write loops that use lots and lots of tokens.  That will cost us money, and could hit the per-day usage limit that is shared by the whole class.\n",
    "\n",
    "Execute one of these cells whenever you want to see your cost so far.  Or, just keep `usage_openai.json` open as a tab in your IDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_usage()      # reads from the file usage_openai.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat usage_openai.json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialogues and dialogue agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this assignment is to create a good \"argubot\" that will talk to people about controversial topics and broaden their minds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first argubot (Airhead)\n",
    "\n",
    "You can have a conversation right now with a _really bad_ argubot named Airhead.  Try asking it about climate change!  When you're done, reply with an empty string.\n",
    "\n",
    "(The `converse()` method calls Python's `input()` function, which will prompt you for input at the command-line or by popping up a box in your IDE.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argubots\n",
    "d = argubots.airhead.converse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *bot* (short for \"robot\") is a system that acts autonomously.\n",
    "That corresponds to the AI notion of an *agent* — a system that uses some *policy* to choose *actions* to take.\n",
    "\n",
    "The `airhead` agent above (defined in `argubots.py`) uses a particularly simple policy.  \n",
    "It is an instance of a simple `Agent` subclass called `ConstantAgent` (defined in `agents.py`).\n",
    "\n",
    "The result of talking to `airhead` is a `Dialogue` object (defined in `dialogue.py`). Let's look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each *turn* of this dialogue is just a tiny dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An LLM argubot (Alice)\n",
    "\n",
    "In other CS courses like crypto, algorithms, or networks, you may have encountered \"conversations\" between characters named Alice and Bob.  \n",
    "Let's try talking to the Alice of this homework, who is a _much stronger baseline_ than Airhead.  Your job in this assignment is to improve upon Alice.\n",
    "We'll meet Bob later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alicechat = argubots.alice.converse()   # or call with argument d if you want to append to the previous conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have guessed, `alice` is powered by an prompted LLM.  You can find the specific prompt in `argubots.py`.\n",
    "\n",
    "So, while `agents.py` provides the core functionality for `Agent` objects, the argubot agents like `alice` -- and the ones that you will write! -- go into `argubots.py` instead.  This is just to keep the files small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating human characters (Bob & friends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll talk to your own argubots to get a qualitative feeling for their strengths and weaknesses.  \n",
    "But can you really be sure you're making progress?  For that, a quantitative measure can be helpful.\n",
    "\n",
    "Ultimately, you should test an argubot like Alice by having it argue with many real humans -- not just you -- and using some rubric to score the resulting dialogues.  But that would be slow and complicated to arrange.  \n",
    "\n",
    "So, meet Bob!  He's just a simulated human.  You won't edit him: he is part of the development set.  Here is some information about him (from `characters.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import characters\n",
    "rich.print(characters.bob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't talk directly to `characters.bob` because that's just a data object.\n",
    "However, you can construct a simple agent that uses that data (plus a few more instructions) to prompt an LLM.\n",
    "\n",
    "(Which LLM does it prompt?  The `CharacterAgent` constructor (defined in `agents.py`) defaults to a GPT-3.5 model that is specified in `tracking.py`.  But you can override that using keyword arguments.)\n",
    "\n",
    "Try talking to Bob about climate change, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import CharacterAgent\n",
    "bob = CharacterAgent(characters.bob)    # actually, agents.bob is already defined this way\n",
    "bob.converse()        # returns a dialogue, but we've already seen it so we don't want to print it again\n",
    "None                  # don't print anything for this notebook cell "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, a proper user study can't just be conducted with one human user.\n",
    "\n",
    "So, meet our bevy of beautiful Bobs!  (They're not actually all named Bob -- we continued on in the alphabet.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agents\n",
    "agents.devset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents.cara.converse()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the underlying character data here in the notebook.  Your argubot will have to deal with all of these topics and styles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.print(characters.devset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating conversation \n",
    "\n",
    "We can make Alice and Bob chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dialogue import Dialogue\n",
    "d = Dialogue()                                              # empty dialogue\n",
    "d = d.add('Alice', \"Do you think it's okay to eat meat?\")   # add first turn\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = agents.bob.respond(d)\n",
    "d = argubots.alice.respond(d)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = agents.bob.respond(d)\n",
    "d = argubots.alice.respond(d)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, let's see what happens when Alice and Bob talk for a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulate import simulated_dialogue\n",
    "d = simulated_dialogue(argubots.alice, agents.bob, 8)\n",
    "rich.print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes this kind of conversation seems to stall out, with Bob in particular repeating himself a lot.  Alice doesn't seem to have a good strategy for getting him to open up.  Maybe you can do a better job talking to Bob, and that will give you some ideas about how to improve Alice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myname = alicechat[0]['speaker']   # your name, pulled from an earlier dialogue\n",
    "agents.bob.converse(d[0:2].rename('Alice', myname))  # reuse the same first two turns\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try talking to the other characters and having Alice (or Airhead) talk to them.\n",
    "\n",
    "**You might enjoy** defining additional characters in `characters.py`, or right here in the notebook.\n",
    "Feel free to talk to those and evaluate them.  They could be variants on the exisiting characters, or something entirely new. \n",
    "\n",
    "However, **don't change the dev set** -- the characters we just loaded must stay the same.  Your job in this homework is to improve the argubot (or at least try).  And that means improving it according to a fixed and stable eval measure.\n",
    "\n",
    "As an exception, you can change the languages that a couple of the characters speak. It may be fun for you to see them try to speak your native language.  And that doesn't really affect the quality of the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "trollFace2 = characters.trollFace.replace(languages = [\"Chinese\", \"Spanish\"])\n",
    "rich.print(trollFace2)\n",
    "simulated_dialogue(argubots.alice, CharacterAgent(trollFace2), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency: Batched generation?\n",
    "\n",
    "Notice that we are making a separate LLM call to generate each turn of the dialogue.  When we generate the $n^\\text{th}$ turn, we send the server the whole dialogue history — the previous $n\\!-\\!1$ turns — along with some instructions.  The server has to re-encode it with the Transformer, and it charges us for doing so (see the \"input token\" costs in `tracking.py`).  \n",
    "\n",
    "That is probably inevitable for real dialogue.  But for simulated dialogue, a more efficient approach would be to generate the whole dialogue between Alice and Bob in one LLM call.  Then you would be charged just once for each dialogue turn.  Under this approach, the Transformer encodes each token as soon as it is generated (see the \"output token\" costs in `tracking.py`).  The encoded token stays in the context throughout the dialogue, so it doesn't have to be re-encoded on a later call.  There is no later call.  \n",
    "\n",
    "Under current pricing models, that would reduce the dollar cost of generating $n$ turns from $O(n^2)$ to $O(n)$.  \n",
    "\n",
    "However, the pricing model doesn't quite reflect the computational costs.  \n",
    "* ![image](handin.png) Using $O(\\cdot)$ notation, what is the total number of floating-point operations needed to generate $n$ turns under each approach?  \n",
    "* ![image](handin.png) Parallelism may help reduce the runtime.  Using $O(\\cdot)$ notation, what is the total number of seconds needed to generate $n$ turns under each approach?  (Assume that the GPU is big enough, relative to $n$, that it can process all input tokens in parallel.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the more efficient approach is that it gives you no way to change the instructions (the system prompt) each time we switch from Alice to Bob and back again.  You'd need to generate the whole conversation using a single set of instructions.\n",
    "\n",
    "![image](handin.png)\n",
    "Can you get this to work?  Specifically, try completing the cell below.  You don't have to use the `Agent` or `Dialogue` classes.  It's okay to just throw together something like the `complete()` method above.  Just see whether you can manage to prompt GPT-3.5 to generate a multi-turn dialogue between two characters who have different personalities and goals.  Is the quality better or worse than generating one turn at a time?  If worse, does it help to switch to GPT-4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like `simulated_dialogue` in `simulate.py`.  However, this one is called on two\n",
    "# Characters, not two Agents, and it returns a string rather than a Dialogue.\n",
    "\n",
    "from tracking import default_client, default_model\n",
    "from characters import Character\n",
    "def simulated_dialogue_batch(a: Character, b: Character, turns: int = 6, *,\n",
    "                             starter=True) -> str:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Try it out!\n",
    "simulated_dialogue_batch(characters.bob, characters.cara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_dialogue(agents.bob, agents.cara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_dialogue(agents.eve, agents.trollFace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-based evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is our goal for the argubot?  We'd like it to broaden the thinking of the (simulated) human that it is talking to.  Indeed, that's what Alice's prompt tells Alice to do.\n",
    "\n",
    "This goal is inspired by the recent paper [Opening up Minds with Argumentative Dialogues](https://aclanthology.org/2022.findings-emnlp.335/), which collected human-human dialogues:\n",
    "\n",
    "> In this work, we focus on argumentative dialogues that aim to open up (rather than change) people’s minds to help them become more understanding to views\n",
    "that are unfamiliar or in opposition to their own convictions. ... Success of the dialogue is measured as the change in the participant’s stance towards those who hold opinions different to theirs.\n",
    "\n",
    "Arguments of this sort are not like chess or tennis games, with an actual winner.  The argubot will almost never hear a human say \"You have convinced me that I was wrong.\"  But the argubot did a good job if the human developed **increased understanding and respect for an opposing point of view**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out whether this happened, we can use a questionnaire to ask the human what they thought after the dialogue.  For example, after Alice talks to Bob, we'll ask Bob to evaluate what he thinks of Alice's views.  Of course, that depends on his personality -- Alice needs to talk to him in a way that reaches *him* (as much as possible).  We'll also ask an outside observer to evaluate whether Alice handled the conversation with Bob well.\n",
    "\n",
    "Of course, we're still not going to use real humans.  Bob is a fake person, and so is the outside observer (whose name is Judge Wise).\n",
    "Using an LLM as an eval metric is known as *model-based evaluation*.  It has pros and cons:\n",
    "* It is cheaper, faster, and more replicable than hiring actual humans to do the evaluation.  \n",
    "* It might give different answers than what humans would give.   \n",
    "\n",
    "Social scientists usually refer to a metric's **reliability** (low variance) and **validity** (low bias).  So the points above say that model-based evaluation is reliable but not necessarily valid.  In general, an LLM-based metric (like any metric) needs to be validated to confirm that it really does measure what it claims to measure.  (For example, that it correlates strongly with some other measure that we already trust.)  In this homework, we'll skip this step and just pray that the metric is reasonable.\n",
    "\n",
    "To see how this works out in practice, open up the `demo` notebook, which walks you through the evaluation protocol.  You'll see how to call the [starter code](http://cs.jhu.edu/~jason/465/hw/llm), how it talks to the LLM behind the scenes, and what it is able to accomplish. \n",
    "\n",
    "To help to validate the metric, check that Airhead gets a low score.  (It should!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the starter code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `demo` notebook gave you a good high-level picture of what the starter code is doing.  So now you're probably curious about the details.  Now that you've had the view from the top, here's a good bottom-up order in which to study the code.  You don't need to understand every detail, but you will need to understand enough to call it and extend it.\n",
    "\n",
    "* `character.py`.  The `Character` class is short and easy.\n",
    "\n",
    "* `dialogue.py`.  The `Dialogue` class is meant to serve as a record of a natural-language conversation among any number of humans and/or agents.  On each *turn* of the dialogue, one of the speakers says something.  \n",
    "\n",
    "   The dialogue's sequence of turns may remind you of the sequence of messages that is sent to OpenAI's chat completions API.  But the OpenAI messages are only labeled with the 4 special roles `user`, `assistant`, `tool`, and `system`.  Those are not quite the same thing as human speakers.  And the OpenAI messages do not necessarily form a natural-language dialogue: some of the messages are dealing with instructions, few-shot prompting, tool use, and so on.  The `agents.dialogue_to_openai` function in the next module will map a `Dialogue` to a (hopefully appropriate) sequence of messages for asking the LLM to extend that dialogue.\n",
    "\n",
    "* `agents.py`.  This module sets up the problem of automatically predicting the next turn in a dialogue, by implementing an `Agent`'s `response()` method.  The `Agent` base class also has some simple convenience methods that you should look at.  \n",
    "\n",
    "   Some important subclasses of `Agent` are defined here as well.  However, you may want to skip over `EvaluationAgent` and come back to it only when you read `eval.py`.\n",
    "\n",
    "* `simulate.py` makes agents talk to one another, which we'll do during evaluation.\n",
    "\n",
    "* `argubots.py` starts to describe some useful agents.  One of them makes use of the `kialo.py` module, which gives access to a database of arguments.\n",
    "\n",
    "* `eval.py` makes use of `simulate.simulated_dialogue` to `agents.EvaluationAgent` to evaluate an argubot.\n",
    "\n",
    "* We also have a couple of utility modules.  These aren't about NLP; look inside if needed.  `logging_cm.py` is what enabled the context manager `with LoggingContext(...):` in the demo notebook.  `tracking.py` sets some global defaults about how to use the OpenAI API, and arranges to track how many tokens we're paying for when you call it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity-based retrieval: Looking up relevant responses\n",
    "\n",
    "Now, it is fine to prompt an LLM to generate text, but there are other methods!\n",
    "There is a long history of machine learning methods that \"memorize\" the training data.\n",
    "To make a prediction or decision at test time, they consult the stored training examples\n",
    "that are most similar to the training situation.\n",
    "\n",
    "_Similarity-based retrieval_ means that given a document $x$, you find the \"most similar\" documents $y \\in Y$, where $Y$ is a given collection of documents.  The most common way to do this is to maximize the _cosine similarity_ $\\vec{e}(x) \\cdot \\vec{e}(y)$, where $\\vec{e}(\\cdot)$ is an embedding function.\n",
    "\n",
    "Should we use the OpenAI embedding model?  We could, but we would have to precompute $\\vec{e}(y)$ for all $y \\in Y$, and store all these vectors in a data structure that supports some type of fast similarity-based search (e.g., using the [FAISS](https://faiss.ai/index.html) package).  An alternative would be to upload the documents to OpenAI and let OpenAI compute and store the embeddings.  We would then use their similarity-based [retrieval tool](https://platform.openai.com/docs/assistants/overview).\n",
    "\n",
    "A simpler and faster approach—which sometimes even works better—is to use a _bag of tokens_ embedding function: Define $\\vec{e}(y)$ to be the vector in $\\mathbb{R}^V$ that records the count of each type of token in a tokenized version of $y$, where $V$ is the token vocabulary.  [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) is a refined variant of that idea, where the counts are adjusted in 3 ways: \n",
    "\n",
    "* smooth the counts\n",
    "* normalize for the document length $|y|$ so that longer documents $y$ are not more likely to be retrieved\n",
    "* downweight tokens that are more common in the corpus (such as ` the` or `ing`) since they provide less information about the content of the document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might like to play with the `rank_bm25` package ([documentation](https://pypi.org/project/rank-bm25/)).  It is widely used and very easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi as BM25_Index   # the standard BM25 method\n",
    "\n",
    "# experiment here!  You could try the examples in the rank_bm25 documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Kialo corpus\n",
    "\n",
    "How can we use similarity-based retrieval to help build an argubot?  It's largely about having the right data!\n",
    "\n",
    "[Kialo](kialo.com) is a collaboratively edited website (like Wikipedia) for discussing political and philosophical topics.  For each topic, the contributors construct a tree of _claims_.  Each claim is a natural-language sentence (usually), and each of its children is another claim that supports it (\"pro\") or opposes it (\"con\").  For example, check out the tree rooted at the claim [\"All humans should be vegan.\"](https://www.kialo.com/all-humans-should-be-vegan-2762).\n",
    "\n",
    "We provide a class `Kialo` for browsing a collection of such trees.  Please read the [source code](https://www.cs.jhu.edu/~jason/465/hw-llm) in `kialo.py`.  The class constructor reads in text files that are [exported Kialo discussions](https://support.kialo.com/en/hc/exporting-a-discussion/); we have provided some in the [data directory](https://www.cs.jhu.edu/~jason/465/hw-llm/data).  The class includes a BM25 index, to be able to find claims that are relevant to a given string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kialo import Kialo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's pull the retrieved discussions (the `.txt` files) into our data structure.\n",
    "\n",
    "For BM25 purposes, we have to be able to turn each document (that is, each Kialo claim) as a list of string or integer tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import glob\n",
    "\n",
    "# kialo = Kialo(glob.glob(\"data/*\"), tokenizer=tokenizer.encode)  # using the LLM's tokenizer doesn't work here for some reason\n",
    "kialo = Kialo(glob.glob(\"data/*\"))  # use simple default tokenizer\n",
    "f\"This Kialo subset contains {len(kialo)} claims\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use sampling to see what kind of stuff is in the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kialo.random_chain()   # just a single random claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kialo.random_chain(n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity-based retrieval from the Kialo corpus\n",
    "\n",
    "Let's try it, using BM25!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kialo.closest_claims(\"animal populations\", n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can restrict to claims for which the Kialo data structure has at least one counterargument (\"con\" child)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kialo.closest_claims(\"animal populations\", n=10, kind='has_cons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = _[0]    # first claim above\n",
    "print(\"Parent claim:\\n\\t\" + str(kialo.parents[c]))\n",
    "print(\"Claim:\\n\\t\" + c)\n",
    "print('\\n\\t* '.join([\"Pro children:\"] + kialo.pros[c]))\n",
    "print('\\n\\t* '.join([\"Con children:\"] + kialo.cons[c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does BM25 really work?\n",
    "\n",
    "![image](handin.png)\n",
    "Unfortunately, we see that `\"animal population\"` gives quite different results from `\"animal populations\"`.  Why is that and how would you fix it?  \n",
    "\n",
    "Also, both queries seem to retrieve some claims that are talking about human populations, not animal populations.  Why is that and how would you fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kialo.closest_claims(\"animal population\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A retrieval bot (Akiko)\n",
    "\n",
    "The starter code defines a simple argubot named Akiko (defined in `argubots.py`) that doesn't use an LLM at all.  It simply finds a Kialo claim that is similar to what the human just said, and responds with one of the Kialo counterarguments to that claim.\n",
    "\n",
    "You already watched Akiko argue with Darius in `demo.py`.  If you look at the log messages, you'll see the claims that Akiko retrieved, as well as the LLM calls that Darius made.  \n",
    "\n",
    "You can talk to Akiko yourself now.  (Remember that Akiko only knows about subjects that it read about in the [`data` directory](https://www.cs.jhu.edu/~jason/465/hw-llm/data/).  If you want to talk about something else, you can add more conversations from [kialo.com]; see the [LICENSE](https://www.cs.jhu.edu/~jason/465/hw-llm/data/LICENSE) file.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging_cm import LoggingContext\n",
    "with LoggingContext(\"agents\", \"INFO\"):   # temporarily increase logging level\n",
    "    argubots.akiko.converse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making your own retrieval bot (Akiki)\n",
    "\n",
    "As you can see when talking to Akiko yourself, Akiko does poorly when responding to a short or vague dialogue turn (like \"Yes\"), because the \"closest claim\" in Kialo may be about a totally different subject.  Akiko does much better at responding to a long and specific statement.  \n",
    "\n",
    "So try implementing a new argubot, called Akiki, that is very much like Akiko but does a better job of staying on topic in such cases.  It should be able to **look at more of the dialogue** than the most recent turn.  But the most recent dialogue turn should still be \"more important\" than earlier turns.  \n",
    "\n",
    "The details are up to you.  Here are a few things you could try:\n",
    "* include earlier dialogue turns in the BM25 query only if the BM25 similarity is too low without them\n",
    "* weight more recent turns more heavily in the BM25 query (how can you arrange that?)\n",
    "* treat the human's earlier turns differently from Akiki's own previous turns\n",
    "\n",
    "![image](handin.png)\n",
    "Implement your new bot in `argubots.py`, and adjust it until `argubots.akiki.converse()` seems to do a better job of answering your short turns, compared to `argubots.akiko.converse()`.  Make sure it still gives appropriate reponses to long turns, too.  Give some examples in the notebook of what worked well and badly, with discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Akiki\n",
    "\n",
    "![image](handin.png)\n",
    "Finally, do a more formal evaluation to verify whether Akiki really does better than Akiko on this dimension.  This is a way to check that you're not just fooling yourself.  \n",
    "\n",
    "1. Make a new `Agent` called \"Shorty\" that often (but not always) gives short responses.  \n",
    "    * Shorty's conversation starters should be on topics that Kialo knows about.  \n",
    "    * Shorty could be a pure `LLMAgent` such as a `CharacterAgent` with a particular `conversational_style`.  Or it could use a mixed strategy of calling the LLM on some turns and not others.\n",
    "2. Generate several *Akiko*-Shorty dialogues and several *Akiki*-Shorty dialogues, using `simulated_dialogue`.\n",
    "3. Evaluate each of those dialogues by asking Judge Wise **whether the argubot stayed on topic**.\n",
    "4. Compare Akiko and Akiki's mean scores.\n",
    "\n",
    "You can do all those steps in the notebook, writing _ad hoc_ code.  You don't have to write general-purpose methods or classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-augmented generation (Aragorn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real weaknesses of Akiko and Akiki:\n",
    "* They can only make statements that are already in Kialo.  \n",
    "* They don't respond to the user's actual statement, but to a single retrieved Kialo claim that may not accurately reflect the user's position (it just overlaps in words).\n",
    "\n",
    "But we also have access to an LLM, which is able to generate new, contextually appropriate text (as Alice does).\n",
    "\n",
    "In this section, you will create an argubot named [Aragorn](https://tolkiengateway.net/wiki/Riddle_of_Strider), who is basically the love child of Akiki and Alice, combining the high-quality specific content of Kialo with the broad competence of an LLM.  \n",
    "\n",
    "The RAG in aRAGorn's name stands for **retrieval-augmented generation**.  Aragorn is an agent that will take 3 steps to compute its `Agent.response()`:\n",
    "\n",
    "1. **Query formation step**: Ask the LLM what claim should be responded to.  For\n",
    "   example, consider the following dialogue:\n",
    "    > ...\n",
    "    > Aragorn: Fortunately, the vaccine was developed in record time.\n",
    "    > Human: Sounds fishy.\n",
    "\n",
    "    \"Sounds fishy\" is exactly the kind of statement that Akiko had trouble using\n",
    "    as a Kialo query.  But Aragorn shows the *whole dialogue* to the LLM, and\n",
    "    asks the LLM what the human's *last turn* was really saying or implying, in\n",
    "    that context. The LLM answers with a much longer statement:\n",
    "\n",
    "    > Human [paraphrased]: A vaccine that was developed very quickly cannot be trusted.\n",
    "    > If its developers are claiming that it is safe and effective, I question their motives.\n",
    "\n",
    "    This paraphrase makes an explicit claim and can be better understood without the context.\n",
    "    It also contains many more word types, which makes it more likely that BM25 will be able\n",
    "    to find a Kialo claim with a nontrivial number of those types. \n",
    "\n",
    "2. **Retrieval step**: Look up claims in Kialo that are similar to the explicit\n",
    "   claim.  Create a short \"document\" that describes some of those claims and\n",
    "   their neighbors on Kialo.\n",
    "\n",
    "3. **Retrieval-augmented generation**: Prompt the LLM to generate the response\n",
    "   (like any `LLMAgent`).  But include the new document somewhere in the LLM\n",
    "   prompt, in a way that it influences the response. \n",
    "   \n",
    "   Thus, the LLM can respond in a way that is appropriate to the dialogue but\n",
    "   also draws on the curated information that was retrieved in Kialo.  After\n",
    "   all, it is a Transformer and can attend to both!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of the kind of document you might create at the retrieval step, though it may be possible\n",
    "to do better than this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refers to global `kialo` as defined above\n",
    "def kialo_responses(s: str) -> str:\n",
    "    c = kialo.closest_claims(s, kind='has_cons')[0]\n",
    "    result = f'One possibly related claim from the Kialo debate website:\\n\\t\"{c}\"'\n",
    "    if kialo.pros[c]:\n",
    "        result += '\\n' + '\\n\\t* '.join([\"Some arguments from other Kialo users in favor of that claim:\"] + kialo.pros[c])\n",
    "    if kialo.cons[c]:\n",
    "        result += '\\n' + '\\n\\t* '.join([\"Some arguments from other Kialo users against that claim:\"] + kialo.cons[c])\n",
    "    return result\n",
    "        \n",
    "print(kialo_responses(\"Animal flesh is yucky to think about, yet delicious.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](handin.png)\n",
    "You should implement Aragorn in `argubots.py`, just as you did for Akiki.  Probably as an instance `aragorn` of a new class `RAGAgent` that is a subclass of `Agent` or `LLMAgent`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Aragorn\n",
    "\n",
    "![image](handin.png)\n",
    "Compare Alice, Akiki, and Aragorn in the notebook, using the evaluation scheme and devset that were illustrated in `demo.ipynb`.  In other words, use `eval.eval_on_characters`.\n",
    "\n",
    "Who does best?  What are the differences in the subscores and comments?  Does it matter which character you're evaluating on — maybe the different characters expoes the bots' various strenghts and weaknesses?\n",
    "\n",
    "Try to figure out how to improve Aragorn's score.  Can you beat Alice?\n",
    "\n",
    "Also, try evaluating them in the same way that you evaluated Akiki.  In other words, have them talk to Shorty and ask Judge Wise whether they were able to stay on topic.  This is where Aragorn should really shine, thanks to its ability to paraphrase Shorty's short utterances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit (Awsom)\n",
    "\n",
    "We didn't require this part this year because the homework is going out late.\n",
    "\n",
    "![image](handinec.png)\n",
    "Add another LLM-based argubot to `argubots.py`.  \n",
    "Call it Awsom.  Try to make it get the best score, according to `eval.eval_on_characters`.\n",
    "Explain what you did and discuss what you found.\n",
    "\n",
    "(This corresponds to the `--awesome` flag on earlier assignments, but naming the character \"Awesome\" might bias the evaluation system, so we changed the spelling!)\n",
    "\n",
    "If the idea was interesting and you implemented it correctly and well, it's okay if it turns out not to help the score.  Many good ideas don't work.  That's why you need to keep finding and trying new good ideas.  (Sometimes they do help, but in a way that is not picked up by the scoring metric.)\n",
    "\n",
    "You may want to use Aragorn or Alice as your starting point.\n",
    "Then see if you can find tricks that will get a more awesome score for Awsom.\n",
    "How you choose to do that is up to you, but some ideas are below.\n",
    "\n",
    "(Reminder: **Don't change evaluation.**  Just build a better argubot.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Extra credit] Prompt engineering\n",
    "\n",
    "A good first thing to do is to experiment with Alice's prompt.  \n",
    "The wording and level of detail in the prompt can be quite important.\n",
    "Often, NLP engineers will change their prompt to try to address \n",
    "problems that they've seen in the responses.\n",
    "\n",
    "Because it's \"just\" text editing, this won't get too much extra credit unless you make a real discovery.\n",
    "But it requires intelligence, care, experimentation, and alertness to the language of the responses and the\n",
    "language of the prompts.  And you'll develop some intuitions about what helps and what doesn't.\n",
    "It is certainly worthwhile.\n",
    "\n",
    "Of course, people have tried to develop methods to search for good prompts automatically, or semi-automatically with human guidance.\n",
    "\n",
    "If you try this, what worked well for you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Extra credit] Chain of thought / Planning\n",
    "\n",
    "The evaluation functions in `eval.py` asked each `EvaluationAgent` a \"warmup question\" before continuing with the real question.  That is an example of chain-of-thought (CoT) reasoning, where the LLM is encouraged to talk through the problem for a few sentences before giving the answer.  CoT sometimes improves performance.\n",
    "\n",
    "Instead of using one prompt, could you help an `LLMAgent` argubot (like Alice) do better by having think aloud before it gives an answer?  For example, each time the human speaks, your argubot (Awsom) could prompt the LLM to think about the human's ideas/motivations/personality, and to come up with a plan for how to open the human's mind. \n",
    "\n",
    "For example, you might structure this as a `Dialogue` among three participants, like this:\n",
    "> Awsom (to Eve): Do you think COVID vaccines should be mandatory?\n",
    ">\n",
    "> Eve: Have you ever gotten vaccinated yourself?<br>\n",
    ">\n",
    "> Awsom (private thought): I don't know Eve's opinions yet, so I can't push back.  Eve might be avoiding my question because she doesn't want to get into a political argument.  So let's see if we can get her to express an opinion on something less political.  Maybe something more personal ... like whether vaccines are scary.\n",
    ">\n",
    "> Awsom (to Eve): In fact I have, and so have millions of others. But some people seem scared about getting the vaccine.  \n",
    "\n",
    "One way to trigger this kind of analysis is to present a `Dialogue.script()` to Awsom (or to an observer), and ask an open-ended question about it.  Or you could ask a series of more specific questions.  That is basically what `eval_by_participant` and `eval_by_observer` do.  But here the argubot itself is doing it, rather than the evaluation framework.\n",
    "\n",
    "Eve would be shown only the turns that are spoken aloud.  However, when analyzing and responding, Awsom would get to see Awsom's own private thoughts as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Extra credit] Dense embeddings\n",
    "\n",
    "BM25 uses sparse embeddings -- a document's embedding vector is mostly zeroes, since the non-zero coordinates correspond to the specific words (tokens) that appear in the document.\n",
    "\n",
    "But perhaps dense embeddings of documents would improve Aragorn by reading the text and abstracting away from the words, in a way that actually cares about word order.  So, try it!\n",
    "\n",
    "How?  As mentioned earlier in this notebook, you could compute the embeddings yourself and put them in a FAISS index. Or you could figure out how to use OpenAI's [knowledge retrieval](https://platform.openai.com/docs/assistants/tools/knowledge-retrieval) API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Extra credit] Few-shot prompting\n",
    "\n",
    " In this homework, often an agent prompted a language model only with instructions.  Can you find a place where giving a few _examples_ would also improve performance?  You will have to write the examples, and you will have to add them to the sequence of messages that your agent to the OpenAI API.  See the sentence=reversal illustration earlier in this notebook.\n",
    "\n",
    "One good opportunity is in the query formation step of RAG.  This is a tricky task.  The LLM is supposed to state the user's implicit claim in a form that looks like a Kialo claim (or, more precisely, a form that will work well as a Kialo query).  It probably doesn't know what Kialo claims look like.  So you could show it by way of example.  This would also show it what you mean by the user's \"implicit claim.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Extra credit] Using tools in the approved way\n",
    "\n",
    "Aragorn's step 1 (query formation) is basically getting the LLM to generate a function call like\n",
    "```\n",
    "kialo_thoughts(\"A vaccine that was developed very quickly ...\")\n",
    "```\n",
    "which Aragorn will execute at step 2 (retrieval), sending the results back to the LLM as part of step 3.\n",
    "\n",
    "In this context, `kialo_thoughts` is an example of a **tool** (that is, a function) that the\n",
    "LLM can or must use before it gives its response.\n",
    "\n",
    "The tool is _not_ something that runs on the LLM server.  It is written by you\n",
    "in Python and executed by you.  The function call above, including the text `\"A\n",
    "vaccine that was ...\"`, is the part that is generated by the LLM.\n",
    "\n",
    "The OpenAI API has [special support](https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models) for calling the LLM in a way that will _allow_ it to generate a tool call ([tools](https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools)) or _force_ it to do so ([tool_choice](https://platform.openai.com/docs/api-reference/chat/create#chat-create-tool_choice)).  You can then send the tool's result back to the LLM [as part of your message sequence](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages).\n",
    "\n",
    "So, you could modify Aragorn to use tools properly.  Maybe that will help, simply because the LLM was trained on message sequences that included tool use.  It should know to pay attention to the tool portions of the prompt when they are relevant, and ignore them when they are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `client.chat.completions.create()` method would need to be told about the tool by using the `tools` keyword argument, with a value something the one below.\n",
    "\n",
    "If `d` is a `Dialogue`, you should be able to call `d.response()` with the `tools` keyword argument.  This will be passed on to `client.chat.completions.create()` as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"kialo_thoughts\",\n",
    "            \"description\": \"Given a claim by the user, find a similar claim on the Kialo website and return its pro and con responses\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_topic\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"A claim that was made explicitly or implicitly by the user.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"search_topic\"],\n",
    "            },\n",
    "        }\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Extra credit] Parallel generation\n",
    "\n",
    "The chat completions interface allows you to sample $n$ continuations of the prompt in parallel, as we saw with \"the apples, bananas, cherries ...\" example.  This is efficient because it requires only 1 request to the LLM server and not $n$.  The latency does not scale with $n$.  Nor does the input token cost, since the prompt only has to be encoded once.\n",
    "\n",
    "Perhaps you can find a way to make use of this?  For example, the query formulation step of RAG could generate $n$ implicit claims instead of just one.  We could then look for claims in the Kialo database that are close to _any_ of those implicit claims.\n",
    "\n",
    "Another thing to do with multiple completions is to select among them or combine them.  For example, suppose we prompt the LLM to generate completions of the form $(s,t,r)$ where $s$ is an answer, $t$ evaluates that answer, and $r$ is a numerical score or reward based on that evaluation.  (\"Write a poem, then tell us about its rhyme and rhythm problems, then give your score.\")  \n",
    "* If we sample multiple completions $(s_1,t_1,r_1), \\ldots, (s_n,t_n,r_n)$ in parallel, then we can return the $s_i$ whose $r_i$ is largest.  \n",
    "* Or if we sample $s$ and then multiple continuations $(t_1,r_1), \\ldots, (t_n,r_n)$, then we can return the mean score $\\sum_i r_i/n$ as a reduced-variance score for $s$, which averages over diverse textual evaluations that might consider different aspects of $s$.\n",
    "\n",
    "Note that when you call the chat completions interface with $n > 1$, you specfy 1 shared input prompt and get $n$ different output completions.  Since the input prompt must be the same for all outputs``, it is necessary to sample all of $(s,t,r)$ or all of $(t,r)$ with a single call to the LLM.\n",
    "\n",
    "Alternatively, it is possible to reduce latency by submitting multiple requests to the server in parallel (see \"async usage\" [here](https://pypi.org/project/openai/)).  In this case the input prompts can be different, although you now have to pay to encode all of them separately.  This facility could speed up evaluation without changing its results; that's a worthwhile thing to try for extra credit!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
