{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick demo of the starter code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't downloaded the starter code and data yet, the command is given at the start of the `hw-llm` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing this cell does some magic.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argubots\n",
    "you_alice = argubots.alice.converse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agents\n",
    "import simulate\n",
    "alice_bob = simulate.simulated_dialogue(argubots.alice, agents.bob, 6)\n",
    "alice_bob   # print it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich\n",
    "\n",
    "# Who is Darius exactly?\n",
    "rich.print(agents.darius.character) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging_cm import LoggingContext\n",
    "\n",
    "# Have Akiko talk to Darius and spy on the back-end messages to/from the LLM server.\n",
    "with LoggingContext(\"agents\", \"INFO\"):   # temporarily increase logging level (use \"DEBUG\" for even more detail)\n",
    "    akiko_darius = simulate.simulated_dialogue(argubots.akiko, agents.darius, 6)\n",
    "\n",
    "rich.print(akiko_darius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import characters\n",
    "from agents import EvaluationAgent\n",
    "import eval\n",
    "\n",
    "rich.print(eval.eval_by_participant(characters.darius, \"Akiko\", akiko_darius))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.print(eval.eval_by_observer(eval.judge, \"Akiko\", akiko_darius))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with LoggingContext(\"agents\", \"INFO\"):   # try that again and watch what's going on under the hood during eval\n",
    "    e1 = eval.eval_by_participant(characters.darius, \"Akiko\", akiko_darius)\n",
    "    e2 = eval.eval_by_observer(eval.default_judge, \"Akiko\", akiko_darius)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.print(\"Eval by participant:\\n\", e1)\n",
    "rich.print(\"Eval by observer:\\n\", e1)\n",
    "rich.print(\"Total eval:\\n\", e1 + e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(e1+e2).mean()   # show just the numeric part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tracking import read_usage\n",
    "read_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"You have spent a total of ${read_usage()['cost']:.2f} of NLP money so far\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with LoggingContext(\"eval\", \"INFO\"):     # makes eval.py show the simulated dialogues and results of evaluation\n",
    "    alice_eval = eval.eval_on_characters(argubots.alice)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "akiko_eval = eval.eval_on_characters(argubots.akiko, reps=1)  # quick eval of the Akiko argubot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although we didn't log the intermediate stuff, it's saved ...\n",
    "from eval import saved_evalsum, saved_dialogues\n",
    "\n",
    "rich.print(saved_evalsum['Akiko'].mean())\n",
    "rich.print(saved_dialogues['Akiko'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick eval of the Eve agent.\n",
    "# She's not designed as an argubot, but maybe her approach would pay off?\n",
    "eve_eval = eval.eval_on_characters(agents.eve, reps=1)  \n",
    "rich.print(saved_evalsum['Eve'].mean())\n",
    "rich.print(saved_dialogues['Eve'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
